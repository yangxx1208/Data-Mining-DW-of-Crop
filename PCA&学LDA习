关于PCA很多书籍、博客都有讲解，讲的都很好，可以看《机器学习》或者下面几个博客的讲解，个人感觉单纯的看周老师西瓜书的可能会比较吃力，
主要是线性代数这方面，本身学习的就不够，对矩阵的理解有待加强。下面几个博客都有很细致的讲解部分涉及矩阵方面的内容：
http://blog.csdn.net/hellotruth/article/details/30750823
http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html
http://blog.csdn.net/zdy0_2004/article/details/50551153
http://www.cnblogs.com/jerrylead/archive/2011/04.html（这个是例子的讲解，方便理解用途和意义，而且这个博客的最小平方误差讲的比较详细）
接下来在谈谈个人的感悟：
其实PCA和LDA很像，归根结底，都是基于方差的一种应用，只不过PCA是无监督学习，LDA是监督学习。
LDA其实是对已标记的样本进行线性变换，讲m维的样本转换成一维，从而通过y判断分类情况。这里面由于是有监督，也就是实现知道类别信息，从而可以通过
类间方差最小化，类类方差最大化来实现（具体实现方法可以先固定其中一个大小，通过极大似然求解）
PCA是通过归并一些冗余或相似的特征、删除一些噪声干扰的特征来达到降维的效果。具体做法也分两种（最小平方误差|投影，最大方差），但推导结果殊途同归。
