关于PCA很多书籍、博客都有讲解，讲的都很好，可以看《机器学习》或者下面几个博客的讲解，个人感觉单纯的看周老师西瓜书的可能会比较吃力，
主要是线性代数这方面，本身学习的就不够，对矩阵的理解有待加强。下面几个博客都有很细致的讲解部分涉及矩阵方面的内容：
http://blog.csdn.net/hellotruth/article/details/30750823
http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html
http://blog.csdn.net/zdy0_2004/article/details/50551153
http://www.cnblogs.com/jerrylead/archive/2011/04.html（这个是例子的讲解，方便理解用途和意义，而且这个博客的最小平方误差讲的比较详细）

接下来在谈谈个人的感悟：
其实PCA和LDA很像，归根结底，都是基于方差实现对样本的特征提取，只不过PCA是无监督学习，LDA是监督学习。
PCA用来提取主要特征信息，这种信息是基于图像样本本身信息的，而LDA则是强调在类别分类上提取主要特征。打个比方，图像的主要信息集中在光照亮度上，此时用PCA提取图像特征，很多都是描述光照的信息，这就可能造成几幅明显不是同一类别的图像，提取的主成分相同，而对于LDA来说，由于提取的是相对于分类的最大特征信息，就会将光照作为影响较小的特征。
LDA其实是对已标记的样本进行线性变换，讲m维的样本转换成一维，从而通过y判断分类情况。这里面由于是有监督，也就是实现知道类别信息，从而可以通过类间方差最大化，类内方差最小化来实现（具体实现方法可以先固定其中一个大小，通过极大似然求解）
PCA是通过归并一些冗余或相似的特征、删除一些噪声干扰的特征来达到降维的效果。具体做法也分两种（最小平方误差|投影，最大方差），但推导结果殊途同归。
